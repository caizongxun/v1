# Google Colab Training Script for ZigZag ML Predictor - FIXED\n# 用於在Google Colab上訓練ZigZag HH/HL/LH/LL預測模型\n# 目標準確率: 80-90%\n\n# !pip install pandas numpy scikit-learn tensorflow xgboost yfinance loguru matplotlib seaborn plotly\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neural_network import MLPClassifier\nimport yfinance as yf\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# =============================================================================\n# PART 1: DATA PREPARATION\n# =============================================================================\n\ndef fetch_market_data(ticker: str = 'BTC-USD', period: str = '2y', interval: str = '1h') -> pd.DataFrame:\n    \"\"\"Fetch market data from Yahoo Finance with proper error handling\"\"\"\n    print(f\"Fetching {ticker} data (period={period}, interval={interval})...\")\n    \n    try:\n        # Download with proper handling\n        df = yf.download(ticker, period=period, interval=interval, progress=False)\n        \n        # Handle MultiIndex columns (when downloading multiple tickers)\n        if isinstance(df.columns, pd.MultiIndex):\n            df.columns = df.columns.get_level_values(0)\n        \n        # Reset index to make date a column\n        df = df.reset_index()\n        \n        # Ensure proper column names\n        df.columns = [col.lower().strip() if isinstance(col, str) else str(col).lower() \n                      for col in df.columns]\n        \n        # Validate required columns exist\n        required_cols = ['open', 'high', 'low', 'close', 'volume']\n        missing_cols = [col for col in required_cols if col not in df.columns]\n        \n        if missing_cols:\n            # Try alternative column names\n            col_map = {\n                'open': ['open', 'Open', 'o'],\n                'high': ['high', 'High', 'h'],\n                'low': ['low', 'Low', 'l'],\n                'close': ['close', 'Close', 'c'],\n                'volume': ['volume', 'Volume', 'v']\n            }\n            \n            for target, alternatives in col_map.items():\n                for alt in alternatives:\n                    if alt in df.columns and target not in df.columns:\n                        df[target] = df[alt]\n                        break\n        \n        # Calculate technical indicators\n        df['Returns'] = df['Close'].pct_change()\n        df['MA7'] = df['Close'].rolling(window=7).mean()\n        df['MA20'] = df['Close'].rolling(window=20).mean()\n        df['Volatility'] = df['Returns'].rolling(window=20).std()\n        \n        df = df.dropna()\n        print(f\"✓ Loaded {len(df)} candles\")\n        print(f\"  Date range: {df['Datetime'].min() if 'Datetime' in df.columns else df.index.min()} to {df['Datetime'].max() if 'Datetime' in df.columns else df.index.max()}\")\n        print(f\"  Price range: {df['Close'].min():.2f} - {df['Close'].max():.2f}\")\n        \n        return df\n    \n    except Exception as e:\n        print(f\"Error fetching data: {e}\")\n        print(\"Creating dummy data for demonstration...\")\n        return create_dummy_data()\n\n\ndef create_dummy_data(n_samples: int = 2000) -> pd.DataFrame:\n    \"\"\"Create dummy OHLCV data for testing\"\"\"\n    print(\"Creating dummy data for testing...\")\n    \n    np.random.seed(42)\n    dates = pd.date_range(end=datetime.now(), periods=n_samples, freq='1H')\n    \n    # Generate realistic OHLCV\n    close = 40000 + np.cumsum(np.random.randn(n_samples) * 100)\n    high = close + np.abs(np.random.randn(n_samples) * 50)\n    low = close - np.abs(np.random.randn(n_samples) * 50)\n    open_ = close + np.random.randn(n_samples) * 50\n    volume = np.random.uniform(100000, 500000, n_samples)\n    \n    df = pd.DataFrame({\n        'Datetime': dates,\n        'Open': open_,\n        'High': high,\n        'Low': low,\n        'Close': close,\n        'Volume': volume\n    })\n    \n    # Add technical indicators\n    df['Returns'] = df['Close'].pct_change()\n    df['MA7'] = df['Close'].rolling(window=7).mean()\n    df['MA20'] = df['Close'].rolling(window=20).mean()\n    df['Volatility'] = df['Returns'].rolling(window=20).std()\n    \n    return df.dropna()\n\n\ndef create_advanced_features(df: pd.DataFrame, lookback: int = 20) -> np.ndarray:\n    \"\"\"\n    Extract advanced features for 80-90% accuracy\n    包含波動率、動量、支撐阻力等特徵\n    \"\"\"\n    print(f\"Extracting advanced features (lookback={lookback})...\")\n    features = []\n    \n    for i in range(lookback, len(df) - 1):\n        window = df.iloc[i-lookback:i+1]\n        \n        try:\n            # 1. Price Statistics\n            returns = window['Close'].pct_change().dropna()\n            high_low_ratio = window['High'] / window['Low']\n            \n            # 2. Volatility Measures\n            volatility = returns.std() if len(returns) > 0 else 0\n            parkinson_volatility = np.sqrt(np.mean(np.log(window['High'] / window['Low'])**2) / (4 * np.log(2))) if len(window) > 0 else 0\n            \n            # 3. Trend Indicators\n            sma_fast = window['Close'].rolling(3).mean().iloc[-1]\n            sma_slow = window['Close'].rolling(10).mean().iloc[-1]\n            trend = (sma_fast / sma_slow - 1) if sma_slow != 0 else 0\n            \n            # 4. Momentum Indicators\n            roc = (window['Close'].iloc[-1] - window['Close'].iloc[0]) / window['Close'].iloc[0] if window['Close'].iloc[0] != 0 else 0\n            \n            # 5. Support/Resistance (Recent highs and lows)\n            recent_high = window['High'].iloc[-5:].max()\n            recent_low = window['Low'].iloc[-5:].min()\n            price_to_resistance = (window['Close'].iloc[-1] / recent_high - 1) if recent_high != 0 else 0\n            price_to_support = (window['Close'].iloc[-1] / recent_low - 1) if recent_low != 0 else 0\n            \n            # 6. Volume Analysis\n            avg_volume = window['Volume'].mean()\n            volume_ratio = window['Volume'].iloc[-1] / avg_volume if avg_volume > 0 else 1\n            \n            # 7. Pattern Detection\n            consecutive_higher_closes = 0\n            consecutive_higher_highs = 0\n            for j in range(1, len(window)):\n                if window['Close'].iloc[j] > window['Close'].iloc[j-1]:\n                    consecutive_higher_closes += 1\n                if window['High'].iloc[j] > window['High'].iloc[j-1]:\n                    consecutive_higher_highs += 1\n            \n            # 8. Volatility Regimes\n            close_std = window['Close'].std()\n            vol_ratio = volatility / close_std if close_std > 0 else 1\n            \n            # 9. ATR-like measure\n            tr1 = window['High'].iloc[-1] - window['Low'].iloc[-1]\n            tr2 = abs(window['High'].iloc[-1] - window['Close'].iloc[-2]) if len(window) > 1 else 0\n            tr3 = abs(window['Low'].iloc[-1] - window['Close'].iloc[-2]) if len(window) > 1 else 0\n            atr = max(tr1, tr2, tr3)\n            atr_ratio = atr / window['Close'].iloc[-1] if window['Close'].iloc[-1] != 0 else 0\n            \n            # 10. Mean Reversion Indicators\n            highest = window['High'].max()\n            lowest = window['Low'].min()\n            position = (window['Close'].iloc[-1] - lowest) / (highest - lowest) if highest > lowest else 0.5\n            \n            # Build feature vector\n            feature_vector = [\n                returns.mean() if len(returns) > 0 else 0,\n                volatility,\n                parkinson_volatility,\n                trend,\n                roc,\n                price_to_resistance,\n                price_to_support,\n                volume_ratio,\n                consecutive_higher_closes,\n                consecutive_higher_highs,\n                vol_ratio,\n                atr_ratio,\n                position,\n                returns.skew() if len(returns) > 0 else 0,\n                returns.kurtosis() if len(returns) > 0 else 0,\n                np.std(high_low_ratio) if len(high_low_ratio) > 0 else 0,\n            ]\n            \n            # Handle NaN/Inf values\n            feature_vector = [0 if (np.isnan(x) or np.isinf(x)) else x for x in feature_vector]\n            features.append(feature_vector)\n        \n        except Exception as e:\n            print(f\"  Warning: Feature extraction error at index {i}: {e}\")\n            features.append([0] * 16)  # Default feature vector\n    \n    result = np.array(features)\n    print(f\"✓ Extracted {result.shape[0]} samples with {result.shape[1]} features\")\n    return result\n\n\ndef prepare_data(df: pd.DataFrame, test_size: float = 0.2):\n    \"\"\"Prepare data for ML training\"\"\"\n    print(\"\\nPreparing data...\")\n    \n    # Extract advanced features\n    X = create_advanced_features(df, lookback=20)\n    \n    # Create labels based on ZigZag logic\n    print(\"Creating ZigZag labels...\")\n    labels = []\n    depth = 12\n    \n    for i in range(depth + 1, len(df) - 1):\n        try:\n            prev_high = df['High'].iloc[max(0, i-depth):i].max()\n            prev_low = df['Low'].iloc[max(0, i-depth):i].min()\n            curr_high = df['High'].iloc[i]\n            curr_low = df['Low'].iloc[i]\n            \n            label = 4  # Default\n            \n            if i > 0:\n                prev_price_high = df['High'].iloc[i-1]\n                prev_price_low = df['Low'].iloc[i-1]\n                \n                if curr_high > prev_high and curr_low > prev_price_low:\n                    label = 0  # HH\n                elif curr_high < prev_high and curr_low > prev_price_low:\n                    label = 1  # HL\n                elif curr_high < prev_high and curr_low < prev_price_low:\n                    label = 2  # LH\n                elif curr_high > prev_high and curr_low < prev_price_low:\n                    label = 3  # LL\n            \n            labels.append(label)\n        except:\n            labels.append(4)\n    \n    y = np.array(labels)\n    \n    print(f\"✓ Labels created\")\n    print(f\"  Label distribution: {pd.Series(y).value_counts().sort_index().to_dict()}\")\n    \n    # Ensure same length\n    min_len = min(len(X), len(y))\n    X = X[:min_len]\n    y = y[:min_len]\n    \n    # Normalize\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    # Train-test split (time series - no shuffle)\n    print(f\"Splitting data (train={1-test_size:.0%}, test={test_size:.0%})...\")\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_scaled, y, test_size=test_size, shuffle=False\n    )\n    \n    # Validation split\n    X_train, X_val, y_train, y_val = train_test_split(\n        X_train, y_train, test_size=0.2, shuffle=False\n    )\n    \n    print(f\"✓ Data split complete\")\n    print(f\"  Train: {X_train.shape[0]} samples\")\n    print(f\"  Val:   {X_val.shape[0]} samples\")\n    print(f\"  Test:  {X_test.shape[0]} samples\")\n    \n    return X_train, X_val, X_test, y_train, y_val, y_test, scaler\n\n\ndef train_ensemble_models(X_train, X_val, y_train, y_val):\n    \"\"\"Train multiple models and ensemble them\"\"\"\n    print(\"\\nTraining ensemble models...\")\n    \n    models = {\n        'RF': RandomForestClassifier(n_estimators=300, max_depth=20, min_samples_split=3, \n                                    class_weight='balanced', random_state=42, n_jobs=-1),\n        'GB': GradientBoostingClassifier(n_estimators=200, learning_rate=0.05, max_depth=8,\n                                        subsample=0.8, random_state=42),\n        'NN': MLPClassifier(hidden_layer_sizes=(256, 128, 64, 32), max_iter=500,\n                           learning_rate_init=0.001, batch_size=32, random_state=42),\n    }\n    \n    trained_models = {}\n    \n    for name, model in models.items():\n        print(f\"  Training {name}...\", end='', flush=True)\n        try:\n            model.fit(X_train, y_train)\n            val_score = model.score(X_val, y_val)\n            trained_models[name] = model\n            print(f\" ✓ Val Accuracy: {val_score:.2%}\")\n        except Exception as e:\n            print(f\" ✗ Error: {e}\")\n    \n    return trained_models\n\n\ndef evaluate_models(models, X_test, y_test):\n    \"\"\"Evaluate ensemble predictions\"\"\"\n    print(\"\\nMaking ensemble predictions...\")\n    \n    predictions_list = []\n    proba_list = []\n    \n    for name, model in models.items():\n        try:\n            if hasattr(model, 'predict_proba'):\n                proba = model.predict_proba(X_test)\n            else:\n                proba = np.eye(5)[model.predict(X_test)]\n            proba_list.append(proba)\n            predictions_list.append(model.predict(X_test))\n        except Exception as e:\n            print(f\"  Warning: Prediction error for {name}: {e}\")\n    \n    if not proba_list:\n        print(\"  Error: No models available for prediction\")\n        return None, None, None\n    \n    # Average predictions\n    avg_proba = np.mean(proba_list, axis=0)\n    ensemble_pred = np.argmax(avg_proba, axis=0)\n    ensemble_conf = np.max(avg_proba, axis=0)\n    \n    return ensemble_pred, ensemble_conf, avg_proba\n\n\n# =============================================================================\n# MAIN EXECUTION\n# =============================================================================\n\ndef main():\n    print(\"=\"*60)\n    print(\"ZigZag ML Predictor - Colab Training Pipeline\")\n    print(\"=\"*60)\n    \n    # Step 1: Fetch data\n    df = fetch_market_data(ticker='BTC-USD', period='2y', interval='1h')\n    \n    # Step 2: Prepare data\n    X_train, X_val, X_test, y_train, y_val, y_test, scaler = prepare_data(df)\n    \n    # Step 3: Train ensemble\n    models = train_ensemble_models(X_train, X_val, y_train, y_val)\n    \n    if not models:\n        print(\"Error: No models trained successfully\")\n        return None, None, None, None\n    \n    # Step 4: Evaluate\n    ensemble_pred, ensemble_conf, avg_proba = evaluate_models(models, X_test, y_test)\n    \n    if ensemble_pred is None:\n        return None, None, None, None\n    \n    # Step 5: Results\n    print(\"\\n\" + \"=\"*60)\n    print(\"RESULTS\")\n    print(\"=\"*60)\n    \n    test_accuracy = accuracy_score(y_test, ensemble_pred)\n    print(f\"\\n✓ Ensemble Test Accuracy: {test_accuracy:.2%}\")\n    print(f\"✓ Average Confidence: {ensemble_conf.mean():.2%}\")\n    \n    print(\"\\nClassification Report:\")\n    print(classification_report(y_test, ensemble_pred, \n                              target_names=['HH', 'HL', 'LH', 'LL', 'No Pattern'],\n                              zero_division=0))\n    \n    # Confusion matrix\n    cm = confusion_matrix(y_test, ensemble_pred)\n    print(\"\\nConfusion Matrix:\")\n    print(cm)\n    \n    # Visualizations\n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n    \n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n                xticklabels=['HH', 'HL', 'LH', 'LL', 'No Pattern'],\n                yticklabels=['HH', 'HL', 'LH', 'LL', 'No Pattern'])\n    axes[0].set_title('Confusion Matrix')\n    axes[0].set_ylabel('True Label')\n    axes[0].set_xlabel('Predicted Label')\n    \n    class_acc = []\n    for i in range(5):\n        mask = y_test == i\n        if mask.sum() > 0:\n            class_acc.append(accuracy_score(y_test[mask], ensemble_pred[mask]))\n        else:\n            class_acc.append(0)\n    \n    axes[1].bar(['HH', 'HL', 'LH', 'LL', 'No Pattern'], class_acc, color='steelblue')\n    axes[1].set_title('Per-Class Accuracy')\n    axes[1].set_ylabel('Accuracy')\n    axes[1].set_ylim([0, 1])\n    for i, v in enumerate(class_acc):\n        axes[1].text(i, v + 0.02, f'{v:.1%}', ha='center')\n    \n    plt.tight_layout()\n    plt.savefig('training_results.png', dpi=100, bbox_inches='tight')\n    print(\"\\n✓ Results saved to training_results.png\")\n    plt.show()\n    \n    # Save model\n    import joblib\n    joblib.dump(scaler, 'zigzag_scaler.pkl')\n    joblib.dump(models, 'zigzag_models.pkl')\n    print(\"\\n✓ Models saved:\")\n    print(\"  - zigzag_scaler.pkl\")\n    print(\"  - zigzag_models.pkl\")\n    \n    print(\"\\n\" + \"=\"*60)\n    print(f\"訓練完成! 準確率: {test_accuracy:.2%}\")\n    print(\"=\"*60)\n    \n    return models, scaler, ensemble_pred, y_test\n\n\nif __name__ == \"__main__\":\n    models, scaler, predictions, true_labels = main()\n    print(\"\\n✓ 準備好上傳到Hugging Face或用於實時交易!\")\n